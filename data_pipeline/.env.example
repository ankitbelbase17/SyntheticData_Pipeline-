# .env.example - Environment Variables Template
# Copy this file to .env and fill in your actual values
# IMPORTANT: Never commit the actual .env file with real credentials to git

# ============================================================================
# AWS S3 CONFIGURATION
# ============================================================================

# AWS S3 Bucket name where images and metadata will be stored
AWS_S3_BUCKET=your-synthetic-data-bucket

# AWS Region (e.g., us-east-1, eu-west-1)
AWS_S3_REGION=us-east-1

# AWS IAM User Access Key ID
AWS_ACCESS_KEY_ID=AKIA1234567890ABCDEF

# AWS IAM User Secret Access Key
AWS_SECRET_ACCESS_KEY=abcdefg1234567890ABCDEFGHIJKLMNOPQRST+/

# ============================================================================
# API KEYS AND TOKENS
# ============================================================================

# Crawlbase API Key
CRAWLBASE_TOKEN=bcDEF_1234XYZ_56789

# HuggingFace API Token (for model downloads)
HF_TOKEN=hf_1234567890abcdefghijklmnopqrstuvwxyz

# OpenAI API Key
OPENAI_API_KEY=sk-1234567890abcdefghijklmnopqrstuvwxyz

# Google Gemini API Key
GEMINI_API_KEY=AIzaSyD1234567890abcdefghijklmnopqrstu

# ============================================================================
# MODEL CONFIGURATION
# ============================================================================

# Device for Qwen VL (cuda or cpu)
QWEN_VL_DEVICE=cuda

# Device for Edit Model (cuda or cpu)
EDIT_MODEL_DEVICE=cuda

# ============================================================================
# SCRAPER CONFIGURATION
# ============================================================================

# Run Zalando scraper in headless mode (True/False)
ZALANDO_SCRAPER_HEADLESS=False

# Maximum pages to scrape (0 = all pages)
ZALANDO_SCRAPER_MAX_PAGES=0

# Maximum items to scrape (0 = no limit)
ZALANDO_SCRAPER_MAX_ITEMS=0

# Delay between requests (seconds)
ZALANDO_SCRAPER_DELAY_MIN=2
ZALANDO_SCRAPER_DELAY_MAX=4

# Save scraper output to S3 (True/False)
ZALANDO_USE_S3=True

# ============================================================================
# LOCAL DIRECTORIES
# ============================================================================

# Local directory for temporary image storage
LOCAL_IMAGES_DIR=./images/

# Local directory for VL analysis outputs
LOCAL_VL_ANALYSIS_DIR=./outputs/vl_analysis/

# Local directory for edited images
LOCAL_EDITED_IMAGES_DIR=./outputs/edited_images/

# Local directory for dataset index
LOCAL_DATASET_INDEX_DIR=./outputs/dataset_index/

# Local directory for scraper output
LOCAL_SCRAPER_OUTPUT_DIR=./vton_gallery_dataset/

# ============================================================================
# LOGGING AND DEBUGGING
# ============================================================================

# Log level (DEBUG, INFO, WARNING, ERROR, CRITICAL)
LOG_LEVEL=INFO

# Log file path
LOG_FILE=logs/data_pipeline.log

# Debug mode (True/False)
DEBUG=False

# ============================================================================
# ENVIRONMENT
# ============================================================================

# Environment (development, staging, production)
ENV=development

# ============================================================================
# NETWORKING (Optional)
# ============================================================================

# Use proxy for requests (True/False)
USE_PROXY=False

# Proxy URL (if USE_PROXY=True)
PROXY_URL=http://proxy.example.com:8080

# ============================================================================
# PERFORMANCE
# ============================================================================

# Number of workers for downloads
NUM_DOWNLOAD_WORKERS=4

# Number of workers for processing
NUM_PROCESSING_WORKERS=2
