version: '3.8'

services:
  synthetic-data-pipeline:
    build:
      context: ./scraper
      dockerfile: Dockerfile
    container_name: synthetic-data-pipeline
    environment:
      - PYTHONUNBUFFERED=1
      - QWEN_VL_DEVICE=cpu
      - HF_TOKEN=${HF_TOKEN:-your_huggingface_token_here}
      - OPENAI_API_KEY=${OPENAI_API_KEY:-your_openai_api_key_here}
      - GEMINI_API_KEY=${GEMINI_API_KEY:-your_gemini_api_key_here}
    volumes:
      - ./scraper:/app
      - ./scraper/images:/app/images
      - ./scraper/outputs:/app/outputs
    ports:
      - "8000:8000"
    networks:
      - pipeline-network
    restart: unless-stopped
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  # Optional: Add a service for monitoring/logging if needed
  # logstash:
  #   image: docker.elastic.co/logstash/logstash:7.14.0
  #   volumes:
  #     - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf
  #   networks:
  #     - pipeline-network
  #   ports:
  #     - "5000:5000"

networks:
  pipeline-network:
    driver: bridge

volumes:
  pipeline-data:
    driver: local
