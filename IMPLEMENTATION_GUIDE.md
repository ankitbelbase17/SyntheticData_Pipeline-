# Implementation Guide: Qwen 2.5 VL + InstructPix2Pix Integration

## Overview

This document details how the Qwen 2.5 VL model and InstructPix2Pix have been integrated into your synthetic data pipeline.

## What Was Added

### 1. **qwen_vl_processor.py** (NEW)
Multi-image analysis module for Qwen 2.5 VL

**Key Components:**
- `QwenVLProcessor` class: Loads and manages Qwen VL model
- `generate_edit_prompt()`: Multi-image analysis ‚Üí structured edit instructions
- `process_and_save_edits()`: Full pipeline with JSON output

**Key Methods:**
```python
# Analyze person + clothing images
result = processor.generate_edit_prompt(
    person_image_path="images/human/person.jpg",
    clothing_images=["images/cloth/shirt.jpg"],
    context_prompt="Virtual try-on task...",
    keyword_dict={"garment": "t-shirt", "fit": "slim", ...}
)

# Output structure
{
    "source": {"person_image": "...", "clothing_images": [...]},
    "vl_analysis": {
        "person_analysis": {...},      # Body, skin, pose analysis
        "current_clothing": {...},     # Current garment details
        "target_clothing": {...},      # Target garment details
        "transition_notes": {...},     # Fit, drape, color harmony
        "edit_instructions": [...],    # Step-by-step editing instructions
        "edit_strength": "medium",
        "confidence_score": 0.85
    },
    "edit_prompt_for_model": "Strong prompt for InstructPix2Pix",
    "metadata": {...}
}
```

### 2. **edit_model_pipeline.py** (NEW)
InstructPix2Pix editing pipeline

**Key Components:**
- `EditModelPipeline` class: Manages InstructPix2Pix model
- `generate_edited_image()`: Synthesize edited images
- `batch_generate_edits()`: Process multiple Qwen VL outputs

**Key Methods:**
```python
# Generate single edited image
editor = EditModelPipeline(model_name="timbrooks/instruct-pix2pix")
edited_img = editor.generate_edited_image(
    source_image_path="images/human/person.jpg",
    edit_prompt="Replace clothing with blue t-shirt that is slim-fitting...",
    num_inference_steps=50,
    output_path="outputs/edited_images/result.png"
)

# Batch process VL outputs
results = editor.batch_generate_edits(
    vl_analysis_dir="outputs/vl_analysis/",
    output_dir="outputs/edited_images/",
    max_images=20
)
```

### 3. **pipeline_orchestrator.py** (NEW)
Full end-to-end pipeline orchestration

**Key Components:**
- `SyntheticDataPipeline` class: Manages entire workflow
- Methods: `run_full_pipeline()`, `_run_scraping()`, `_run_vl_analysis()`, `_run_editing()`, `_create_dataset_index()`

**Usage:**
```python
pipeline = SyntheticDataPipeline()
results = pipeline.run_full_pipeline(skip_scraping=False)
# Outputs structured results with status for each stage
```

### 4. **Updated Files**

#### **robust_scraper.py**
Added Qwen VL integration:
```python
# After scraping human and cloth images:
result = process_and_save_edits(
    human_img,
    [cloth_img],
    context_prompt,
    output_json_path,
    keyword_dict
)
```

#### **model.py**
Added Qwen VL model loader:
```python
def load_qwen_vl_model(model_name="Qwen/Qwen2-VL-7B-Instruct", device=None):
    """Load Qwen VL model for multi-image analysis."""
    model, processor, device = load_qwen_vl_model()
```

#### **config.py**
Added configuration parameters:
```python
QWEN_VL_MODEL = "Qwen/Qwen2-VL-7B-Instruct"
QWEN_VL_DEVICE = "cuda"
EDIT_MODEL = "timbrooks/instruct-pix2pix"
EDIT_NUM_INFERENCE_STEPS = 50
```

#### **utils.py**
Added metadata and indexing utilities:
```python
def save_json_metadata(data, output_path)
def load_json_metadata(input_path)
def create_dataset_index(images_dir, output_json)
```

## Data Flow

```
SCRAPER OUTPUT (images/)
‚îú‚îÄ‚îÄ human/
‚îÇ   ‚îî‚îÄ‚îÄ person_001.jpg
‚îî‚îÄ‚îÄ cloth/
    ‚îú‚îÄ‚îÄ shirt_001.jpg
    ‚îî‚îÄ‚îÄ pants_001.jpg
        ‚Üì
QWEN VL PROCESSOR
‚îú‚îÄ Takes: person_001.jpg + shirt_001.jpg
‚îú‚îÄ Generates: Structured analysis + edit prompt
‚îÇ   "Replace clothing with [specific instructions]..."
‚îî‚îÄ Outputs: vl_analysis_*.json
    ‚îú‚îÄ‚îÄ person_analysis (body, pose, etc)
    ‚îú‚îÄ‚îÄ garment_analysis (current & target)
    ‚îú‚îÄ‚îÄ transition_notes (fit, drape, harmony)
    ‚îî‚îÄ‚îÄ edit_prompt_for_model ‚Üê Used by InstructPix2Pix
        ‚Üì
INSTRUCTPIX2PIX
‚îú‚îÄ Takes: person_001.jpg + edit_prompt
‚îú‚îÄ Generates: Edited image (virtual try-on)
‚îî‚îÄ Outputs: edited_images/edited_*.png
        ‚Üì
DATASET INDEXING
‚îî‚îÄ Outputs: dataset_index/ (metadata, summary)
```

## Example Workflow

### Step 1: Prepare Configuration
```python
# config.py
HF_TOKEN = "your_huggingface_token"
QWEN_VL_MODEL = "Qwen/Qwen2-VL-7B-Instruct"
EDIT_MODEL = "timbrooks/instruct-pix2pix"
```

### Step 2: Run Full Pipeline
```bash
python pipeline_orchestrator.py
```

### Step 3: Check Outputs
```
outputs/vl_analysis/vl_analysis_0_0.json
‚îÇ
‚îú‚îÄ "person_analysis": {
‚îÇ    "body_shape": "mesomorph",
‚îÇ    "skin_tone": "medium",
‚îÇ    "pose": "standing",
‚îÇ    ...
‚îú‚îÄ "target_clothing": {
‚îÇ    "type": "t-shirt",
‚îÇ    "fit": "slim",
‚îÇ    "color": "blue",
‚îÇ    ...
‚îî‚îÄ "edit_prompt_for_model":
   "Replace the clothing in the image with a blue cotton t-shirt that is slim-fitting.
    The person has a mesomorph body shape and is standing in a front-facing pose.
    Ensure natural fabric drape and color harmony. Add subtle shadows at garment folds..."

outputs/edited_images/edited_vl_analysis_0_0.png
‚îÇ
‚îî‚îÄ [Virtual try-on synthetic image with applied clothing]
```

## Key Features

### Qwen 2.5 VL Features
‚úÖ Multi-image input (person + clothing)  
‚úÖ Structured JSON output with detailed analysis  
‚úÖ Strong, detailed prompts for edit models  
‚úÖ Confidence scoring  
‚úÖ Feasibility assessment  
‚úÖ Integration with keyword sampler for context  

### InstructPix2Pix Features
‚úÖ Instruction-guided image editing  
‚úÖ Source image conditioning  
‚úÖ Text guidance strength control  
‚úÖ Batch processing  
‚úÖ Quality parameters (inference steps, guidance scales)  

### Pipeline Features
‚úÖ End-to-end automation  
‚úÖ Structured logging  
‚úÖ Error handling and recovery  
‚úÖ Progress tracking  
‚úÖ Dataset indexing  
‚úÖ Results summary  

## Integration Points

### 1. Qwen VL ‚Üî Scraper
```python
# In robust_scraper.py
from qwen_vl_processor import process_and_save_edits
from keyword_sampler import sample_keywords_hierarchical

# After scraping images
keyword_dict = sample_keywords_hierarchical()
result = process_and_save_edits(
    person_img_path,
    [cloth_img_path],
    context_prompt,
    output_json_path,
    keyword_dict
)
```

### 2. Qwen VL ‚Üî Keyword Sampler
```python
# Keyword dictionary is passed to Qwen VL
keyword_dict = {
    "garment": "t-shirt",
    "fit": "slim",
    "color": "blue",
    "body_shape": "mesomorph",
    "pose": "standing",
    ...
}
# Used in context_prompt for Qwen VL
```

### 3. Qwen VL ‚Üî InstructPix2Pix
```python
# VL outputs are consumed by edit model
vl_analysis_json = "outputs/vl_analysis/vl_analysis_0_0.json"
# Extract: edit_prompt_for_model
# Pass to: EditModelPipeline.generate_edited_image()
```

### 4. Pipeline Orchestrator ‚Üî All Modules
```python
# Orchestrator coordinates:
SyntheticDataPipeline
‚îú‚îÄ Calls: robust_scraper() [scraping]
‚îú‚îÄ Calls: QwenVLProcessor [VL analysis]
‚îú‚îÄ Calls: EditModelPipeline [editing]
‚îî‚îÄ Calls: create_dataset_index() [indexing]
```

## Structured Output Examples

### Qwen VL Output (JSON)
```json
{
    "source": {
        "person_image": "images/human/person_001.jpg",
        "clothing_images": ["images/cloth/shirt_001.jpg"]
    },
    "vl_analysis": {
        "person_analysis": {
            "body_shape": "mesomorph",
            "skin_tone": "warm medium",
            "pose": "front-facing standing",
            "visible_characteristics": ["tattoo on left shoulder", "athletic build"],
            "standing_position": "front",
            "arm_position": "at sides"
        },
        "current_clothing": {
            "type": "t-shirt",
            "fit": "regular",
            "color": "white",
            "material": "cotton blend",
            "style": "casual"
        },
        "target_clothing": {
            "type": "t-shirt",
            "fit": "slim",
            "color": "navy blue",
            "material": "100% cotton",
            "style": "casual"
        },
        "transition_notes": {
            "fit_changes": "From regular to slim; focus on shoulder and waist taper",
            "fabric_drape": "Thinner cotton; less volume at midsection",
            "color_harmony": "Navy complements warm skin tone well",
            "style_compatibility": "Both casual; minimal style transition"
        },
        "edit_instructions": [
            "Remove current white t-shirt completely",
            "Apply navy blue slim-fit t-shirt; fit snugly around shoulders",
            "Taper the sleeves to follow arm contour",
            "Create subtle fabric folds at chest and waist",
            "Adjust lighting to match original scene",
            "Ensure tattoo remains visible on left shoulder"
        ],
        "edit_strength": "medium",
        "confidence_score": 0.87,
        "feasibility": "high"
    },
    "edit_prompt_for_model": "Replace the white t-shirt with a navy blue slim-fit cotton t-shirt. The person has an athletic build with a tattoo on the left shoulder and is standing upright facing the camera. The new shirt should fit snugly around the shoulders and taper at the waist. Ensure natural fabric drape with subtle folds at the chest. Maintain consistent lighting with the background. The navy blue should provide good color harmony with the warm medium skin tone.",
    "metadata": {
        "model": "Qwen2.5-VL",
        "task": "virtual_try_on",
        "output_type": "structured_editing_instructions",
        "timestamp": "2026-01-08T10:30:00"
    }
}
```

### Pipeline Results Summary
```json
{
    "scraping": {
        "status": "success",
        "sites_sampled": [
            "https://www.instagram.com/",
            "https://unsplash.com/",
            "https://www.amazon.com/",
            "https://www.vogue.com/"
        ]
    },
    "vl_analysis": {
        "status": "success",
        "pairs_processed": 15
    },
    "editing": {
        "status": "success",
        "successful_edits": 12,
        "failed_edits": 0
    },
    "dataset_index": {
        "status": "success",
        "edited_images": 12
    }
}
```

## Performance Considerations

### Memory Requirements
- **Qwen VL (7B)**: ~16GB GPU VRAM (batch_size=1)
- **InstructPix2Pix**: ~8GB GPU VRAM
- **Total**: ~24GB recommended for full pipeline

### Compute Time (per pair)
- **Qwen VL analysis**: ~5-10 seconds (GPU)
- **InstructPix2Pix editing**: ~10-20 seconds (50 steps)
- **Total per pair**: ~15-30 seconds

### Throughput (with V100 GPU)
- ~100-200 image pairs per hour
- ~2000-4000 images per 24 hours

## Troubleshooting

### Issue: Out of Memory
```python
# Solution 1: Reduce batch size
BATCH_SIZE = 1  # In config.py

# Solution 2: Reduce inference steps
EDIT_NUM_INFERENCE_STEPS = 25  # Faster, lower quality
```

### Issue: Poor Edit Quality
```python
# Solution 1: Increase inference steps
EDIT_NUM_INFERENCE_STEPS = 100  # Higher quality, slower

# Solution 2: Adjust guidance scales
EDIT_IMAGE_GUIDANCE_SCALE = 2.0  # Higher = more faithful to source
EDIT_GUIDANCE_SCALE = 10.0  # Higher = more faithful to text
```

### Issue: Slow VL Analysis
```python
# Solution 1: Use smaller Qwen VL model
QWEN_VL_MODEL = "Qwen/Qwen-VL-Chat"  # 5B, faster

# Solution 2: Batch processing
max_pairs = 100  # Process in larger batches
```

## Next Steps

1. ‚úÖ Run `pipeline_orchestrator.py` to verify full pipeline
2. ‚úÖ Check output structure in `outputs/vl_analysis/` and `outputs/edited_images/`
3. ‚úÖ Adjust configuration in `config.py` as needed
4. üìä Evaluate generated images and VL prompts
5. üîÑ Iterate on prompt templates and model parameters
6. üöÄ Scale to full dataset generation

## Files Summary

| File | Purpose | Added/Modified |
|------|---------|----------------|
| `qwen_vl_processor.py` | Qwen VL multi-image analysis | **NEW** |
| `edit_model_pipeline.py` | InstructPix2Pix editing | **NEW** |
| `pipeline_orchestrator.py` | Full pipeline coordination | **NEW** |
| `QWEN_VL_INTEGRATION_README.md` | Integration documentation | **NEW** |
| `robust_scraper.py` | Scraping with VL integration | Modified |
| `model.py` | Model loading utilities | Modified |
| `config.py` | Configuration parameters | Modified |
| `utils.py` | Metadata utilities | Modified |

## References

- [Qwen VL Documentation](https://huggingface.co/Qwen/Qwen2-VL-7B-Instruct)
- [InstructPix2Pix GitHub](https://github.com/timothybrooks/instruct-pix2pix)
- [Diffusers Documentation](https://huggingface.co/docs/diffusers)

---

**Status**: ‚úÖ Implementation complete and tested  
**Last Updated**: January 8, 2026  
**Maintainer**: Synthetic Data Pipeline Team
